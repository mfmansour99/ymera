# ===============================================================================
# logging/audit_logger.py
# ===============================================================================

"""
YMERA Enterprise - Audit Logger
Production-Ready Audit Trail Management - v4.0
Enterprise-grade implementation with zero placeholders
"""

# ===============================================================================
# STANDARD IMPORTS SECTION
# ===============================================================================

import asyncio
import json
import uuid
import hashlib
import hmac
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Callable
from dataclasses import dataclass, field, asdict
from abc import ABC, abstractmethod
from enum import Enum
import threading
from contextlib import asynccontextmanager

# Third-party imports  
import structlog
from pydantic import BaseModel, Field, validator
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.backends import default_backend
import base64
import os

# Local imports
from config.settings import get_settings
from utils.encryption import encrypt_sensitive_data, decrypt_sensitive_data
from monitoring.performance_tracker import track_performance
from .structured_logger import get_logger, LogLevel
from .log_formatters import AuditFormatter

# ===============================================================================
# LOGGING CONFIGURATION
# ===============================================================================

logger = structlog.get_logger(f"ymera.{__name__.split('.')[-1]}")

# ===============================================================================
# CONSTANTS & CONFIGURATION
# ===============================================================================

# Audit event types
class AuditEventType(Enum):
    # Authentication events
    LOGIN_SUCCESS = "auth.login.success"
    LOGIN_FAILURE = "auth.login.failure"
    LOGOUT = "auth.logout"
    SESSION_EXPIRED = "auth.session.expired"
    PASSWORD_CHANGE = "auth.password.change"
    ACCOUNT_LOCKED = "auth.account.locked"
    
    # Authorization events
    ACCESS_GRANTED = "authz.access.granted"
    ACCESS_DENIED = "authz.access.denied"
    PERMISSION_GRANTED = "authz.permission.granted"
    PERMISSION_REVOKED = "authz.permission.revoked"
    ROLE_ASSIGNED = "authz.role.assigned"
    ROLE_REMOVED = "authz.role.removed"
    
    # Data events
    DATA_CREATE = "data.create"
    DATA_READ = "data.read"
    DATA_UPDATE = "data.update"
    DATA_DELETE = "data.delete"
    DATA_EXPORT = "data.export"
    DATA_IMPORT = "data.import"
    
    # System events
    SYSTEM_START = "system.start"
    SYSTEM_STOP = "system.stop"
    SYSTEM_CONFIG_CHANGE = "system.config.change"
    SYSTEM_BACKUP = "system.backup"
    SYSTEM_RESTORE = "system.restore"
    
    # Security events
    SECURITY_BREACH_DETECTED = "security.breach.detected"
    SECURITY_SCAN_COMPLETED = "security.scan.completed"
    SECURITY_POLICY_VIOLATION = "security.policy.violation"
    INTRUSION_DETECTED = "security.intrusion.detected"
    
    # Compliance events
    COMPLIANCE_CHECK = "compliance.check"
    COMPLIANCE_VIOLATION = "compliance.violation"
    COMPLIANCE_REPORT_GENERATED = "compliance.report.generated"
    
    # Administrative events
    ADMIN_ACTION = "admin.action"
    USER_CREATED = "admin.user.created"
    USER_DELETED = "admin.user.deleted"
    USER_MODIFIED = "admin.user.modified"

# Audit outcome types
class AuditOutcome(Enum):
    SUCCESS = "success"
    FAILURE = "failure"
    PARTIAL = "partial"
    UNKNOWN = "unknown"

# Compliance frameworks
class ComplianceFramework(Enum):
    SOX =# ===============================================================================
# logging/log_handlers.py
# ===============================================================================

"""
YMERA Enterprise - Log Handlers
Production-Ready Log Handler Implementations - v4.0
Enterprise-grade implementation with zero placeholders
"""

# ===============================================================================
# STANDARD IMPORTS SECTION
# ===============================================================================

import asyncio
import logging
import json
import os
import uuid
import smtplib
import gzip
import aiofiles
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Callable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from pathlib import Path
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import ssl
from urllib.parse import urlparse
import queue
import threading
from collections import deque
import time

# Third-party imports
import aiohttp
import redis.asyncio as aioredis
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy import Column, String, DateTime, Integer, Text, JSON
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import boto3
from elasticsearch import AsyncElasticsearch

# Local imports
from config.settings import get_settings
from utils.encryption import encrypt_data, decrypt_data
from monitoring.performance_tracker import track_performance

# ===============================================================================
# LOGGING CONFIGURATION
# ===============================================================================

import structlog
logger = structlog.get_logger(f"ymera.{__name__.split('.')[-1]}")

# ===============================================================================
# CONSTANTS & CONFIGURATION
# ===============================================================================

# Handler constants
MAX_RETRY_ATTEMPTS = 3
RETRY_DELAY_SECONDS = 1
MAX_BUFFER_SIZE = 10000
FLUSH_TIMEOUT_SECONDS = 30
BATCH_SIZE = 100

settings = get_settings()

# Database model for log storage
Base = declarative_base()

class LogRecord(Base):
    """Database model for log records"""
    __tablename__ = "log_records"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    timestamp = Column(DateTime, nullable=False)
    level = Column(String(20), nullable=False)
    logger_name = Column(String(255), nullable=False)
    module = Column(String(255), nullable=False)
    function = Column(String(255), nullable=False)
    line_number = Column(Integer, nullable=False)
    message = Column(Text, nullable=False)
    user_id = Column(String(255), nullable=True)
    session_id = Column(String(255), nullable=True)
    correlation_id = Column(String(255), nullable=True)
    request_id = Column(String(255), nullable=True)
    extra_data = Column(JSON, nullable=True)
    stack_trace = Column(Text, nullable=True)
    performance_metrics = Column(JSON, nullable=True)
    security_context = Column(JSON, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)

# ===============================================================================
# BASE HANDLER CLASS
# ===============================================================================

class BaseLogHandler(ABC):
    """Abstract base class for all log handlers"""
    
    def __init__(self, 
                 min_level: 'LogLevel' = None,
                 buffer_size: int = 0,
                 flush_interval: int = 5,
                 enable_retry: bool = True,
                 max_retries: int = MAX_RETRY_ATTEMPTS):
        self.min_level = min_level
        self.buffer_size = buffer_size
        self.flush_interval = flush_interval
        self.enable_retry = enable_retry
        self.max_retries = max_retries
        
        # Buffer management
        self._buffer = deque(maxlen=buffer_size if buffer_size > 0 else None)
        self._buffer_lock = asyncio.Lock()
        self._flush_task = None
        self._shutdown_event = asyncio.Event()
        
        # Metrics
        self._metrics = {
            "total_handled": 0,
            "total_errors": 0,
            "buffer_overflows": 0,
            "retry_attempts": 0,
            "last_flush": None
        }
        
        # Initialize handler
        self._initialize()
    
    @abstractmethod
    async def _initialize(self) -> None:
        """Initialize handler-specific resources"""
        pass
    
    @abstractmethod
    async def _write_log(self, entry: 'LogEntrySchema') -> None:
        """Write a single log entry"""
        pass
    
    @abstractmethod
    async def _write_batch(self, entries: List['LogEntrySchema']) -> None:
        """Write a batch of log entries"""
        pass
    
    async def handle(self, entry: 'LogEntrySchema') -> None:
        """Handle a log entry"""
        try:
            # Check minimum level
            if self.min_level and self._get_level_value(entry.level) < self._get_level_value(self.min_level.name):
                return
            
            # Update metrics
            self._metrics["total_handled"] += 1
            
            # Handle buffering
            if self.buffer_size > 0:
                await self._add_to_buffer(entry)
            else:
                await self._process_entry(entry)
                
        except Exception as e:
            self._metrics["total_errors"] += 1
            logger.error("Handler error", handler=self.__class__.__name__, error=str(e))
    
    async def _add_to_buffer(self, entry: 'LogEntrySchema') -> None:
        """Add entry to buffer"""
        async with self._buffer_lock:
            if len(self._buffer) >= self.buffer_size:
                self._metrics["buffer_overflows"] += 1
                self._buffer.popleft()  # Remove oldest entry
            
            self._buffer.append(entry)
            
            # Start flush task if not running
            if not self._flush_task:
                self._flush_task = asyncio.create_task(self._flush_worker())
    
    async def _flush_worker(self) -> None:
        """Background worker for flushing buffer"""
        while not self._shutdown_event.is_set():
            try:
                await asyncio.wait_for(
                    self._shutdown_event.wait(),
                    timeout=self.flush_interval
                )
            except asyncio.TimeoutError:
                pass
            
            await self._flush_buffer()
    
    async def _flush_buffer(self) -> None:
        """Flush buffer contents"""
        if not self._buffer:
            return
        
        async with self._buffer_lock:
            entries = list(self._buffer)
            self._buffer.clear()
        
        if entries:
            try:
                await self._write_batch(entries)
                self._metrics["last_flush"] = datetime.utcnow()
            except Exception as e:
                self._metrics["total_errors"] += 1
                logger.error("Buffer flush error", error=str(e))
    
    async def _process_entry(self, entry: 'LogEntrySchema') -> None:
        """Process a single entry with retry logic"""
        last_error = None
        
        for attempt in range(self.max_retries + 1 if self.enable_retry else 1):
            try:
                await self._write_log(entry)
                return
                
            except Exception as e:
                last_error = e
                self._metrics["retry_attempts"] += 1
                
                if attempt < self.max_retries and self.enable_retry:
                    await asyncio.sleep(RETRY_DELAY_SECONDS * (attempt + 1))
                    continue
                else:
                    break
        
        # If we get here, all retries failed
        self._metrics["total_errors"] += 1
        logger.error("Handler write failed after retries", 
                    error=str(last_error), 
                    attempts=self.max_retries + 1)
    
    def _get_level_value(self, level_name: str) -> int:
        """Get numeric value for log level"""
        level_values = {
            "TRACE": 5, "DEBUG": 10, "INFO": 20, "WARNING": 30,
            "ERROR": 40, "CRITICAL": 50, "SECURITY": 60, "AUDIT": 70
        }
        return level_values.get(level_name.upper(), 20)
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get handler metrics"""
        return self._metrics.copy()
    
    async def flush(self) -> None:
        """Manually flush handler"""
        await self._flush_buffer()
    
    async def cleanup(self) -> None:
        """Cleanup handler resources"""
        try:
            # Signal shutdown
            self._shutdown_event.set()
            
            # Flush remaining entries
            await self._flush_buffer()
            
            # Wait for flush task to complete
            if self._flush_task:
                try:
                    await asyncio.wait_for(self._flush_task, timeout=5)
                except asyncio.TimeoutError:
                    self._flush_task.cancel()
            
            logger.info("Handler cleanup completed", handler=self.__class__.__name__)
            
        except Exception as e:
            logger.error("Handler cleanup error", error=str(e))

# ===============================================================================
# CONCRETE HANDLER IMPLEMENTATIONS
# ===============================================================================

class AsyncFileHandler(BaseLogHandler):
    """Asynchronous file handler with rotation support"""
    
    def __init__(self, 
                 filename: Path,
                 max_bytes: int = MAX_LOG_SIZE,
                 backup_count: int = MAX_BACKUP_COUNT,
                 enable_compression: bool = True,
                 enable_encryption: bool = False,
                 **kwargs):
        self.filename = Path(filename)
        self.max_bytes = max_bytes
        self.backup_count = backup_count
        self.enable_compression = enable_compression
        self.enable_encryption = enable_encryption
        
        # Ensure directory exists
        self.filename.parent.mkdir(parents=True, exist_ok=True)
        
        # File management
        self._file_lock = asyncio.Lock()
        self._current_size = 0
        self._file_handle = None
        
        super().__init__(**kwargs)
    
    async def _initialize(self) -> None:
        """Initialize file handler"""
        try:
            # Get current file size if exists
            if self.filename.exists():
                self._current_size = self.filename.stat().st_size
            
            logger.info("File handler initialized", 
                       filename=str(self.filename),
                       current_size=self._current_size)
        except Exception as e:
            logger.error("Failed to initialize file handler", error=str(e))
            raise
    
    async def _write_log(self, entry: 'LogEntrySchema') -> None:
        """Write single log entry to file"""
        from .log_formatters import JSONFormatter
        
        formatter = JSONFormatter(compact=True)
        log_line = formatter.format(entry) + '\n'
        
        async with self._file_lock:
            await self._check_rotation()
            
            async with aiofiles.open(self.filename, 'a', encoding='utf-8') as f:
                if self.enable_encryption:
                    log_line = encrypt_data(log_line)
                
                await f.write(log_line)
                await f.flush()
                
                self._current_size += len(log_line.encode('utf-8'))
    
    async def _write_batch(self, entries: List['LogEntrySchema']) -> None:
        """Write batch of log entries to file"""
        from .log_formatters import JSONFormatter
        
        formatter = JSONFormatter(compact=True)
        log_lines = []
        
        for entry in entries:
            log_line = formatter.format(entry) + '\n'
            log_lines.append(log_line)
        
        batch_content = ''.join(log_lines)
        
        async with self._file_lock:
            await self._check_rotation()
            
            async with aiofiles.open(self.filename, 'a', encoding='utf-8') as f:
                if self.enable_encryption:
                    batch_content = encrypt_data(batch_content)
                
                await f.write(batch_content)
                await f.flush()
                
                self._current_size += len(batch_content.encode('utf-8'))
    
    async def _check_rotation(self) -> None:
        """Check if file rotation is needed"""
        if self._current_size >= self.max_bytes:
            await self._rotate_files()
    
    async def _rotate_files(self) -> None:
        """Rotate log files"""
        try:
            # Close current file if open
            if self._file_handle:
                await self._file_handle.close()
                self._file_handle = None
            
            # Rotate existing backup files
            for i in range(self.backup_count - 1, 0, -1):
                old_file = self.filename.with_suffix(f'.{i}')
                new_file = self.filename.with_suffix(f'.{i + 1}')
                
                if old_file.exists():
                    if new_file.exists():
                        new_file.unlink()
                    old_file.rename(new_file)
            
            # Move current file to .1
            if self.filename.exists():
                backup_file = self.filename.with_suffix('.1')
                if backup_file.exists():
                    backup_file.unlink()
                
                # Compress if enabled
                if self.enable_compression:
                    compressed_file = self.filename.with_suffix('.1.gz')
                    await self._compress_file(self.filename, compressed_file)
                    self.filename.unlink()
                else:
                    self.filename.rename(backup_file)
            
            # Reset current size
            self._current_size = 0
            
            logger.info("File rotation completed", filename=str(self.filename))
            
        except Exception as e:
            logger.error("File rotation failed", error=str(e))
            raise
    
    async def _compress_file(self, source: Path, destination: Path) -> None:
        """Compress a file using gzip"""
        async with aiofiles.open(source, 'rb') as f_in:
            content = await f_in.read()
        
        compressed_content = gzip.compress(content)
        
        async with aiofiles.open(destination, 'wb') as f_out:
            await f_out.write(compressed_content)

class RemoteLogHandler(BaseLogHandler):
    """Handler for sending logs to remote endpoints"""
    
    def __init__(self, 
                 endpoint: str,
                 api_key: Optional[str] = None,
                 headers: Optional[Dict[str, str]] = None,
                 timeout: int = 30,
                 **kwargs):
        self.endpoint = endpoint
        self.api_key = api_key
        self.headers = headers or {}
        self.timeout = timeout
        
        # HTTP session
        self._session = None
        
        super().__init__(**kwargs)
    
    async def _initialize(self) -> None:
        """Initialize HTTP session"""
        try:
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            self._session = aiohttp.ClientSession(timeout=timeout)
            
            # Set default headers
            if self.api_key:
                self.headers['Authorization'] = f'Bearer {self.api_key}'
            
            self.headers['Content-Type'] = 'application/json'
            
            logger.info("Remote handler initialized", endpoint=self.endpoint)
            
        except Exception as e:
            logger.error("Failed to initialize remote handler", error=str(e))
            raise
    
    async def _write_log(self, entry: 'LogEntrySchema') -> None:
        """Send single log entry to remote endpoint"""
        payload = entry.dict()
        payload['timestamp'] = payload['timestamp'].isoformat()
        
        async with self._session.post(
            self.endpoint,
            json=payload,
            headers=self.headers
        ) as response:
            if response.status >= 400:
                error_text = await response.text()
                raise Exception(f"Remote logging failed: {response.status} - {error_text}")
    
    async def _write_batch(self, entries: List['LogEntrySchema']) -> None:
        """Send batch of log entries to remote endpoint"""
        payload = []
        for entry in entries:
            entry_dict = entry.dict()
            entry_dict['timestamp'] = entry_dict['timestamp'].isoformat()
            payload.append(entry_dict)
        
        batch_payload = {
            "logs": payload,
            "batch_id": str(uuid.uuid4()),
            "batch_size": len(payload),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        async with self._session.post(
            f"{self.endpoint}/batch",
            json=batch_payload,
            headers=self.headers
        ) as response:
            if response.status >= 400:
                error_text = await response.text()
                raise Exception(f"Remote batch logging failed: {response.status} - {error_text}")
    
    async def cleanup(self) -> None:
        """Cleanup HTTP session"""
        await super().cleanup()
        if self._session:
            await self._session.close()

class DatabaseLogHandler(BaseLogHandler):
    """Handler for storing logs in database"""
    
    def __init__(self, 
                 database_url: str,
                 table_name: str = "log_records",
                 pool_size: int = 10,
                 max_overflow: int = 20,
                 **kwargs):
        self.database_url = database_url
        self.table_name = table_name
        self.pool_size = pool_size
        self.max_overflow = max_overflow
        
        # Database resources
        self._engine = None
        self._session_factory = None
        
        super().__init__(**kwargs)
    
    async def _initialize(self) -> None:
        """Initialize database connection"""
        try:
            self._engine = create_async_engine(
                self.database_url,
                pool_size=self.pool_size,
                max_overflow=self.max_overflow,
                echo=False
            )
            
            self._session_factory = sessionmaker(
                self._engine,
                class_=AsyncSession,
                expire_on_commit=False
            )
            
            # Create tables if needed
            async with self._engine.begin() as conn:
                await conn.run_sync(Base.metadata.create_all)
            
            logger.info("Database handler initialized", database_url=self.database_url)
            
        except Exception as e:
            logger.error("Failed to initialize database handler", error=str(e))
            raise
    
    async def _write_log(self, entry: 'LogEntrySchema') -> None:
        """Store single log entry in database"""
        async with self._session_factory() as session:
            log_record = LogRecord(
                timestamp=entry.timestamp,
                level=entry.level,
                logger_name=entry.logger_name,
                module=entry.module,
                function=entry.function,
                line_number=entry.line_number,
                message=entry.message,
                user_id=entry.user_id,
                session_id=entry.session_id,
                correlation_id=entry.correlation_id,
                request_id=entry.request_id,
                extra_data=entry.extra_data,
                stack_trace=entry.stack_trace,
                performance_metrics=entry.performance_metrics,
                security_context=entry.security_context
            )
            
            session.add(log_record)
            await session.commit()
    
    async def _write_batch(self, entries: List['LogEntrySchema']) -> None:
        """Store batch of log entries in database"""
        async with self._session_factory() as session:
            log_records = []
            
            for entry in entries:
                log_record = LogRecord(
                    timestamp=entry.timestamp,
                    level=entry.level,
                    logger_name=entry.logger_name,
                    module=entry.module,
                    function=entry.function,
                    line_number=entry.line_number,
                    message=entry.message,
                    user_id=entry.user_id,
                    session_id=entry.session_id,
                    correlation_id=entry.correlation_id,
                    request_id=entry.request_id,
                    extra_data=entry.extra_data,
                    stack_trace=entry.stack_trace,
                    performance_metrics=entry.performance_metrics,
                    security_context=entry.security_context
                )
                log_records.append(log_record)
            
            session.add_all(log_records)
            await session.commit()
    
    async def cleanup(self) -> None:
        """Cleanup database resources"""
        await super().cleanup()
        if self._engine:
            await self._engine.dispose()

class SlackNotificationHandler(BaseLogHandler):
    """Handler for sending critical logs to Slack"""
    
    def __init__(self, 
                 webhook_url: str,
                 channel: Optional[str] = None,
                 username: str = "YMERA-Logger",
                 **kwargs):
        self.webhook_url = webhook_url
        self.channel = channel
        self.username = username
        
        # HTTP session
        self._session = None
        
        super().__init__(**kwargs)
    
    async def _initialize(self) -> None:
        """Initialize Slack handler"""
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            self._session = aiohttp.ClientSession(timeout=timeout)
            
            logger.info("Slack handler initialized")
            
        except Exception as e:
            logger.error("Failed to initialize Slack handler", error=str(e))
            raise
    
    async def _write_log(self, entry: 'LogEntrySchema') -> None:
        """Send log entry to Slack"""
        color_map = {
            "ERROR": "#ff0000",
            "CRITICAL": "#8b0000",
            "SECURITY": "#ff6600",
            "WARNING": "#ffff00"
        }
        
        attachment = {
            "color": color_map.get(entry.level, "#36a64f"),
            "title": f"{entry.level} - {entry.logger_name}",
            "text": entry.message,
            "fields": [
                {
                    "title": "Module",
                    "value": f"{entry.module}:{entry.function}:{entry.line_number}",
                    "short": True
                },
                {
                    "title": "Timestamp",
                    "value": entry.timestamp.isoformat(),
                    "short": True
                }
            ]
        }
        
        if entry.user_id:
            attachment["fields"].append({
                "title": "User",
                "value": entry.user_id,
                "short": True
            })
        
        if entry.correlation_id:
            attachment["fields"].append({
                "title": "Correlation ID",
                "value": entry.correlation_id,
                "short": True
            })
        
        if entry.stack_trace:
            attachment["fields"].append({
                "title": "Stack Trace",
                "value": f"```{entry.stack_trace[:1000]}```",
                "short": False
            })
        
        payload = {
            "username": self.username,
            "attachments": [attachment]
        }
        
        if self.channel:
            payload["channel"] = self.channel
        
        async with self._session.post(self.webhook_url, json=payload) as response:
            if response.status >= 400:
                error_text = await response.text()
                raise Exception(f"Slack notification failed: {response.status} - {error_text}")
    
    async def _write_batch(self, entries: List['LogEntrySchema']) -> None:
        """Send multiple entries as single Slack message"""
        if not entries:
            return
        
        # Group by level
        by_level = {}
        for entry in entries:
            if entry.level not in by_level:
                by_level[entry.level] = []
            by_level[entry.level].append(entry)
        
        attachments = []
        color_map = {
            "ERROR": "#ff0000",
            "CRITICAL": "#8b0000", 
            "SECURITY": "#ff6600",
            "WARNING": "#ffff00"
        }
        
        for level, level_entries in by_level.items():
            attachment = {
                "color": color_map.get(level, "#36a64f"),
                "title": f"Batch Alert - {level} ({len(level_entries)} events)",
                "text": f"Multiple {level} events occurred",
                "fields": []
            }
            
            # Add sample entries
            for i, entry in enumerate(level_entries[:3]):  # Show first 3
                attachment["fields"].append({
                    "title": f"Event {i+1}",
                    "value": f"{entry.message[:100]}...",
                    "short": False
                })
            
            if len(level_entries) > 3:
                attachment["fields"].append({
                    "title": "Additional Events",
                    "value": f"... and {len(level_entries) - 3} more events",
                    "short": False
                })
            
            attachments.append(attachment)
        
        payload = {
            "username": self.username,
            "attachments": attachments
        }
        
        if self.channel:
            payload["channel"] = self.channel
        
        async with self._session.post(self.webhook_url, json=payload) as response:
            if response.status >= 400:
                error_text = await response.text()
                raise Exception(f"Slack batch notification failed: {response.status} - {error_text}")
    
    async def cleanup(self) -> None:
        """Cleanup HTTP session"""
        await super().cleanup()
        if self._session:
            await self._session.close()

class EmailAlertHandler(BaseLogHandler):
    """Handler for sending critical logs via email"""
    
    def __init__(self, 
                 config: Dict[str, Any],
                 **kwargs):
        """
        Initialize email handler
        
        config should contain:
        - smtp_server: SMTP server hostname
        - smtp_port: SMTP server port 
        - username: SMTP username
        - password: SMTP password
        - from_address: From email address
        - to_addresses: List of recipient addresses
        - use_tls: Whether to use TLS
        """
        self.smtp_server = config['smtp_server']
        self.smtp_port = config['smtp_port']
        self.username = config['username']
        self.password = config['password']
        self.from_address = config['from_address']
        self.to_addresses = config['to_addresses']
        self.use_tls = config.get('use_tls', True)
        
        super().__init__(**kwargs)
    
    async def _initialize(self) -> None:
        """Initialize email handler"""
        logger.info("Email handler initialized", 
                   smtp_server=self.smtp_server,
                   recipients=len(self.to_addresses))
    
    async def _write_log(self, entry: 'LogEntrySchema') -> None:
        """Send log entry via email"""
        subject = f"YMERA Alert - {entry.level} in {entry.module}"
        
        # Create email content
        body = f"""
YMERA System Alert

Level: {entry.level}
Time: {entry.timestamp.isoformat()}
Logger: {entry.logger_name}
Module: {entry.module}:{entry.function}:{entry.line_number}
Message: {entry.message}

Context:
"""
        
        if entry.user_id:
            body += f"User ID: {entry.user_id}\n"
        if entry.session_id:
            body += f"Session ID: {entry.session_id}\n"
        if entry.correlation_id:
            body += f"Correlation ID: {entry.correlation_id}\n"
        if entry.request_id:
            body += f"Request ID: {entry.request_id}\n"
        
        if entry.extra_data:
            body += f"\nExtra Data:\n{json.dumps(entry.extra_data, indent=2)}\n"
        
        if entry.stack_trace:
            body += f"\nStack Trace:\n{entry.stack_trace}\n"
        
        await self._send_email(subject, body)
    
    async def _write_batch(self, entries: List['LogEntrySchema']) -> None:
        """Send batch of entries as single email"""
        if not entries:
            return
        
        subject = f"YMERA Batch Alert - {len(entries)} events"
        
        body = f"YMERA System Batch Alert\n\n{len(entries)} events occurred:\n\n"
        
        for i, entry in enumerate(entries, 1):
            body += f"Event {i}:\n"
            body += f"  Level: {entry.level}\n"
            body += f"  Time: {entry.timestamp.isoformat()}\n"
            body += f"  Module: {entry.module}:{entry.function}\n"
            body += f"  Message: {entry.message}\n"
            if entry.user_id:
                body += f"  User: {entry.user_id}\n"
            body += "\n"
        
        await self._send_email(subject, body)
    
    async def _send_email(self, subject: str, body: str) -> None:
        """Send email using SMTP"""
        def send_sync():
            msg = MIMEMultipart()
            msg['From'] = self.from_address
            msg['To'] = ', '.join(self.to_addresses)
            msg['Subject'] = subject
            
            msg.attach(MIMEText(body, 'plain'))
            
            try:
                server = smtplib.SMTP(self.smtp_server, self.smtp_port)
                
                if self.use_tls:
                    context = ssl.create_default_context()
                    server.starttls(context=context)
                
                server.login(self.username, self.password)
                server.send_message(msg)
                server.quit()
                
            except Exception as e:
                raise Exception(f"Email sending failed: {str(e)}")
        
        # Run in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, send_sync)

class MetricsHandler(BaseLogHandler):
    """Handler for collecting log metrics"""
    
    def __init__(self, **kwargs):
        self._metrics_data = {
            "total_logs": 0,
            "logs_by_level": {},
            "logs_by_module": {},
            "logs_by_logger": {},
            "error_rate": 0.0,
            "performance_stats": {
                "avg_response_time": 0.0,
                "slow_operations": []
            },
            "security_events": 0,
            "audit_events": 0
        }
        
        super().__init__(**kwargs)
    
    async def _initialize(self) -> None:
        """Initialize metrics handler"""
        logger.info("Metrics handler initialized")
    
    async def _write_log(self, entry: 'LogEntrySchema') -> None:
        """Process log entry for metrics"""
        self._metrics_data["total_logs"] += 1
        
        # Count by level
        level = entry.level
        self._metrics_data["logs_by_level"][level] = (
            self._metrics_data["logs_by_level"].get(level, 0) + 1
        )
        
        # Count by module
        module = entry.module
        self._metrics_data["logs_by_module"][module] = (
            self._metrics_data["logs_by_module"].get(module, 0) + 1
        )
        
        # Count by logger
        logger_name = entry.logger_name
        self._metrics_data["logs_by_logger"][logger_name] = (
            self._metrics_data["logs_by_logger"].get(logger_name, 0) + 1
        )
        
        # Special event counts
        if level == "SECURITY":
            self._metrics_data["security_events"] += 1
        elif level == "AUDIT":
            self._metrics_data["audit_events"] += 1
        
        # Performance metrics
        if entry.performance_metrics and 'duration_seconds' in entry.performance_metrics:
            duration = entry.performance_metrics['duration_seconds']
            
            # Track slow operations (> 1 second)
            if duration > 1.0:
                slow_op = {
                    "operation": entry.function,
                    "module": entry.module,
                    "duration": duration,
                    "timestamp": entry.timestamp.isoformat()
                }
                
                self._metrics_data["performance_stats"]["slow_operations"].append(slow_op)
                
                # Keep only last 100 slow operations
                if len(self._metrics_data["performance_stats"]["slow_operations"]) > 100:
                    self._metrics_data["performance_stats"]["slow_operations"].pop(0)
        
        # Calculate error rate
        total_logs = self._metrics_data["total_logs"]
        error_logs = (
            self._metrics_data["logs_by_level"].get("ERROR", 0) +
            self._metrics_data["logs_by_level"].get("CRITICAL", 0)
        )
        self._metrics_data["error_rate"] = (error_logs / total_logs) * 100 if total_logs > 0 else 0.0
    
    async def _write_batch(self, entries: List['LogEntrySchema']) -> None:
        """Process batch of entries for metrics"""
        for entry in entries:
            await self._write_log(entry)
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get current metrics"""
        return self._metrics_data.copy()
    
    def reset_metrics(self) -> None:
        """Reset all metrics"""
        self._metrics_data = {
            "total_logs": 0,
            "logs_by_level": {},
            "logs_by_module": {},
            "logs_by_logger": {},
            "error_rate": 0.0,
            "performance_stats": {
                "avg_response_time": 0.0,
                "slow_operations": []
            },
            "security_events": 0,
            "audit_events": 0
        }

# ===============================================================================
# UTILITY FUNCTIONS
# ===============================================================================

def create_handler(handler_type: str, **kwargs) -> BaseLogHandler:
    """Factory function to create handlers"""
    handlers = {
        "file": AsyncFileHandler,
        "remote": RemoteLogHandler,
        "database": DatabaseLogHandler,
        "slack": SlackNotificationHandler,
        "email": EmailAlertHandler,
        "metrics": MetricsHandler
    }
    
    if handler_type not in handlers:
        raise ValueError(f"Unknown handler type: {handler_type}")
    
    return handlers[handler_type](**kwargs)

async def test_handler(handler: BaseLogHandler) -> Dict[str, Any]:
    """Test a handler with sample log entry"""
    from ..structured_logger import LogEntrySchema
    
    test_entry = LogEntrySchema(
        timestamp=datetime.utcnow(),
        level="INFO",
        message="Handler test message",
        logger_name="test_logger",
        module="test_module",
        function="test_function",
        line_number=123,
        thread_id="test_thread",
        process_id=12345
    )
    
    try:
        await handler.handle(test_entry)
        await handler.flush()
        
        return {
            "status": "success",
            "metrics": handler.get_metrics()
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

# ===============================================================================
# EXPORTS
# ===============================================================================

__all__ = [
    "BaseLogHandler",
    "AsyncFileHandler",
    "RemoteLogHandler",
    "DatabaseLogHandler",
    "SlackNotificationHandler",
    "EmailAlertHandler",
    "MetricsHandler",
    "LogRecord",
    "create_handler",
    "test_handler"
]"""
YMERA Enterprise - Logging Infrastructure
Production-Ready Comprehensive Logging System - v4.0
Enterprise-grade implementation with zero placeholders
"""

# ===============================================================================
# logging/__init__.py
# ===============================================================================

"""
YMERA Enterprise Logging Package
Comprehensive logging infrastructure with structured logging, audit trails,
and enterprise-grade monitoring capabilities.
"""

from .structured_logger import (
    StructuredLogger,
    LoggerManager,
    get_logger,
    configure_logging,
    LogLevel,
    LogConfig
)
from .log_formatters import (
    JSONFormatter,
    ConsoleFormatter,
    AuditFormatter,
    SecurityFormatter,
    PerformanceFormatter
)
from .log_handlers import (
    AsyncFileHandler,
    RemoteLogHandler,
    DatabaseLogHandler,
    SlackNotificationHandler,
    EmailAlertHandler,
    MetricsHandler
)
from .audit_logger import (
    AuditLogger,
    AuditEvent,
    AuditEventType,
    SecurityAuditLogger,
    ComplianceAuditLogger
)

__version__ = "4.0.0"
__all__ = [
    # Core logging
    "StructuredLogger",
    "LoggerManager", 
    "get_logger",
    "configure_logging",
    "LogLevel",
    "LogConfig",
    # Formatters
    "JSONFormatter",
    "ConsoleFormatter",
    "AuditFormatter",
    "SecurityFormatter",
    "PerformanceFormatter",
    # Handlers
    "AsyncFileHandler",
    "RemoteLogHandler",
    "DatabaseLogHandler",
    "SlackNotificationHandler",
    "EmailAlertHandler",
    "MetricsHandler",
    # Audit logging
    "AuditLogger",
    "AuditEvent",
    "AuditEventType",
    "SecurityAuditLogger",
    "ComplianceAuditLogger"
]

# ===============================================================================
# logging/structured_logger.py
# ===============================================================================

"""
YMERA Enterprise - Structured Logger
Production-Ready Structured Logging Implementation - v4.0
Enterprise-grade implementation with zero placeholders
"""

# ===============================================================================
# STANDARD IMPORTS SECTION
# ===============================================================================

import asyncio
import logging
import json
import os
import uuid
import sys
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Callable, AsyncGenerator
from dataclasses import dataclass, field, asdict
from abc import ABC, abstractmethod
from contextlib import asynccontextmanager
from pathlib import Path
from enum import Enum
import threading
from concurrent.futures import ThreadPoolExecutor

# Third-party imports
import structlog
from structlog.stdlib import LoggerFactory
from structlog.processors import JSONRenderer, TimeStamper, add_log_level, StackInfoRenderer
import redis.asyncio as aioredis
from pydantic import BaseModel, Field, validator

# Local imports
from config.settings import get_settings
from utils.encryption import encrypt_sensitive_data
from monitoring.performance_tracker import track_performance

# ===============================================================================
# LOGGING CONFIGURATION
# ===============================================================================

logger = structlog.get_logger(f"ymera.{__name__.split('.')[-1]}")

# ===============================================================================
# CONSTANTS & CONFIGURATION
# ===============================================================================

# Logging levels
class LogLevel(Enum):
    TRACE = 5
    DEBUG = 10
    INFO = 20
    WARNING = 30
    ERROR = 40
    CRITICAL = 50
    SECURITY = 60
    AUDIT = 70

# Configuration constants
MAX_LOG_SIZE = 100 * 1024 * 1024  # 100MB
MAX_BACKUP_COUNT = 10
LOG_ROTATION_INTERVAL = 24 * 3600  # 24 hours
BUFFER_SIZE = 1000
FLUSH_INTERVAL = 5  # seconds

settings = get_settings()

# ===============================================================================
# DATA MODELS & SCHEMAS
# ===============================================================================

@dataclass
class LogConfig:
    """Configuration for logging system"""
    level: LogLevel = LogLevel.INFO
    format_type: str = "json"
    enable_console: bool = True
    enable_file: bool = True
    enable_remote: bool = False
    log_directory: Path = Path("logs")
    max_file_size: int = MAX_LOG_SIZE
    backup_count: int = MAX_BACKUP_COUNT
    buffer_size: int = BUFFER_SIZE
    flush_interval: int = FLUSH_INTERVAL
    enable_encryption: bool = False
    enable_compression: bool = True
    enable_audit: bool = True
    enable_metrics: bool = True
    remote_endpoint: Optional[str] = None
    database_url: Optional[str] = None
    slack_webhook: Optional[str] = None
    email_config: Optional[Dict[str, Any]] = None

@dataclass
class LogEntry:
    """Structured log entry"""
    timestamp: datetime
    level: LogLevel
    message: str
    logger_name: str
    module: str
    function: str
    line_number: int
    thread_id: str
    process_id: int
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    correlation_id: Optional[str] = None
    request_id: Optional[str] = None
    extra_data: Dict[str, Any] = field(default_factory=dict)
    stack_trace: Optional[str] = None
    performance_metrics: Optional[Dict[str, float]] = None
    security_context: Optional[Dict[str, Any]] = None

class LogEntrySchema(BaseModel):
    """Pydantic schema for log entry validation"""
    timestamp: datetime
    level: str
    message: str
    logger_name: str
    module: str
    function: str
    line_number: int
    thread_id: str
    process_id: int
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    correlation_id: Optional[str] = None
    request_id: Optional[str] = None
    extra_data: Dict[str, Any] = Field(default_factory=dict)
    stack_trace: Optional[str] = None
    performance_metrics: Optional[Dict[str, float]] = None
    security_context: Optional[Dict[str, Any]] = None
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat(),
            uuid.UUID: lambda v: str(v)
        }

# ===============================================================================
# CORE IMPLEMENTATION CLASSES
# ===============================================================================

class StructuredLogger:
    """Production-ready structured logger with comprehensive features"""
    
    def __init__(self, name: str, config: LogConfig):
        self.name = name
        self.config = config
        self._logger = None
        self._handlers = []
        self._formatters = {}
        self._buffer = []
        self._buffer_lock = threading.Lock()
        self._flush_task = None
        self._shutdown_event = threading.Event()
        self._metrics = {
            "total_logs": 0,
            "logs_by_level": {},
            "errors_count": 0,
            "last_flush": None
        }
        
        self._initialize_logger()
        self._start_background_tasks()
    
    def _initialize_logger(self) -> None:
        """Initialize the structured logger with all components"""
        try:
            # Configure structlog
            structlog.configure(
                processors=[
                    structlog.stdlib.filter_by_level,
                    structlog.stdlib.add_logger_name,
                    structlog.stdlib.add_log_level,
                    structlog.stdlib.PositionalArgumentsFormatter(),
                    structlog.processors.TimeStamper(fmt="iso"),
                    structlog.processors.StackInfoRenderer(),
                    structlog.processors.format_exc_info,
                    self._add_context_processor,
                    JSONRenderer() if self.config.format_type == "json" else structlog.dev.ConsoleRenderer()
                ],
                context_class=dict,
                logger_factory=LoggerFactory(),
                wrapper_class=structlog.stdlib.BoundLogger,
                cache_logger_on_first_use=True,
            )
            
            self._logger = structlog.get_logger(self.name)
            
            # Setup handlers based on configuration
            self._setup_handlers()
            
            logger.info("Structured logger initialized", logger_name=self.name)
            
        except Exception as e:
            print(f"Failed to initialize logger {self.name}: {str(e)}")
            raise
    
    def _add_context_processor(self, logger, method_name, event_dict):
        """Add contextual information to log entries"""
        # Add correlation ID if available
        correlation_id = getattr(threading.current_thread(), 'correlation_id', None)
        if correlation_id:
            event_dict['correlation_id'] = correlation_id
        
        # Add request ID if available
        request_id = getattr(threading.current_thread(), 'request_id', None)
        if request_id:
            event_dict['request_id'] = request_id
        
        # Add user context if available
        user_context = getattr(threading.current_thread(), 'user_context', None)
        if user_context:
            event_dict['user_id'] = user_context.get('user_id')
            event_dict['session_id'] = user_context.get('session_id')
        
        # Add performance metrics if available
        perf_metrics = getattr(threading.current_thread(), 'performance_metrics', None)
        if perf_metrics:
            event_dict['performance_metrics'] = perf_metrics
        
        return event_dict
    
    def _setup_handlers(self) -> None:
        """Setup logging handlers based on configuration"""
        from .log_handlers import (
            AsyncFileHandler, RemoteLogHandler, DatabaseLogHandler,
            SlackNotificationHandler, EmailAlertHandler, MetricsHandler
        )
        
        # File handler
        if self.config.enable_file:
            file_handler = AsyncFileHandler(
                filename=self.config.log_directory / f"{self.name}.log",
                max_bytes=self.config.max_file_size,
                backup_count=self.config.backup_count
            )
            self._handlers.append(file_handler)
        
        # Remote handler
        if self.config.enable_remote and self.config.remote_endpoint:
            remote_handler = RemoteLogHandler(
                endpoint=self.config.remote_endpoint,
                buffer_size=self.config.buffer_size
            )
            self._handlers.append(remote_handler)
        
        # Database handler
        if self.config.database_url:
            db_handler = DatabaseLogHandler(
                database_url=self.config.database_url,
                table_name=f"logs_{self.name}"
            )
            self._handlers.append(db_handler)
        
        # Notification handlers for critical logs
        if self.config.slack_webhook:
            slack_handler = SlackNotificationHandler(
                webhook_url=self.config.slack_webhook,
                min_level=LogLevel.ERROR
            )
            self._handlers.append(slack_handler)
        
        if self.config.email_config:
            email_handler = EmailAlertHandler(
                config=self.config.email_config,
                min_level=LogLevel.CRITICAL
            )
            self._handlers.append(email_handler)
        
        # Metrics handler
        if self.config.enable_metrics:
            metrics_handler = MetricsHandler()
            self._handlers.append(metrics_handler)
    
    def _start_background_tasks(self) -> None:
        """Start background tasks for log processing"""
        if self.config.buffer_size > 0:
            self._flush_task = threading.Thread(
                target=self._flush_worker,
                daemon=True
            )
            self._flush_task.start()
    
    def _flush_worker(self) -> None:
        """Background worker for flushing log buffer"""
        while not self._shutdown_event.is_set():
            try:
                self._shutdown_event.wait(timeout=self.config.flush_interval)
                if self._buffer:
                    self._flush_buffer()
            except Exception as e:
                print(f"Error in flush worker: {str(e)}")
    
    def _flush_buffer(self) -> None:
        """Flush buffered log entries to handlers"""
        with self._buffer_lock:
            if not self._buffer:
                return
            
            buffer_copy = self._buffer.copy()
            self._buffer.clear()
        
        # Process buffered entries
        for entry in buffer_copy:
            self._process_log_entry(entry, buffered=False)
        
        self._metrics["last_flush"] = datetime.utcnow()
    
    def _process_log_entry(self, entry: LogEntry, buffered: bool = True) -> None:
        """Process a single log entry through all handlers"""
        try:
            # Update metrics
            self._metrics["total_logs"] += 1
            level_name = entry.level.name
            self._metrics["logs_by_level"][level_name] = (
                self._metrics["logs_by_level"].get(level_name, 0) + 1
            )
            
            # Convert to schema for validation
            entry_dict = asdict(entry)
            entry_dict["level"] = entry.level.name
            validated_entry = LogEntrySchema(**entry_dict)
            
            # Process through handlers
            for handler in self._handlers:
                try:
                    asyncio.create_task(handler.handle(validated_entry))
                except Exception as e:
                    self._metrics["errors_count"] += 1
                    print(f"Handler error: {str(e)}")
                    
        except Exception as e:
            self._metrics["errors_count"] += 1
            print(f"Error processing log entry: {str(e)}")
    
    def _create_log_entry(
        self,
        level: LogLevel,
        message: str,
        **kwargs
    ) -> LogEntry:
        """Create a structured log entry"""
        # Get caller information
        frame = sys._getframe(3)  # Go up the call stack
        
        return LogEntry(
            timestamp=datetime.utcnow(),
            level=level,
            message=message,
            logger_name=self.name,
            module=frame.f_globals.get("__name__", "unknown"),
            function=frame.f_code.co_name,
            line_number=frame.f_lineno,
            thread_id=str(threading.current_thread().ident),
            process_id=os.getpid(),
            user_id=kwargs.get("user_id"),
            session_id=kwargs.get("session_id"),
            correlation_id=kwargs.get("correlation_id"),
            request_id=kwargs.get("request_id"),
            extra_data=kwargs.get("extra_data", {}),
            stack_trace=kwargs.get("stack_trace"),
            performance_metrics=kwargs.get("performance_metrics"),
            security_context=kwargs.get("security_context")
        )
    
    # Public logging methods
    def trace(self, message: str, **kwargs) -> None:
        """Log trace level message"""
        if self.config.level.value <= LogLevel.TRACE.value:
            entry = self._create_log_entry(LogLevel.TRACE, message, **kwargs)
            self._add_to_buffer(entry)
    
    def debug(self, message: str, **kwargs) -> None:
        """Log debug level message"""
        if self.config.level.value <= LogLevel.DEBUG.value:
            entry = self._create_log_entry(LogLevel.DEBUG, message, **kwargs)
            self._add_to_buffer(entry)
    
    def info(self, message: str, **kwargs) -> None:
        """Log info level message"""
        if self.config.level.value <= LogLevel.INFO.value:
            entry = self._create_log_entry(LogLevel.INFO, message, **kwargs)
            self._add_to_buffer(entry)
    
    def warning(self, message: str, **kwargs) -> None:
        """Log warning level message"""
        if self.config.level.value <= LogLevel.WARNING.value:
            entry = self._create_log_entry(LogLevel.WARNING, message, **kwargs)
            self._add_to_buffer(entry)
    
    def error(self, message: str, **kwargs) -> None:
        """Log error level message"""
        if self.config.level.value <= LogLevel.ERROR.value:
            # Add stack trace for errors
            if "stack_trace" not in kwargs:
                kwargs["stack_trace"] = traceback.format_exc()
            entry = self._create_log_entry(LogLevel.ERROR, message, **kwargs)
            self._add_to_buffer(entry)
    
    def critical(self, message: str, **kwargs) -> None:
        """Log critical level message"""
        if self.config.level.value <= LogLevel.CRITICAL.value:
            # Add stack trace for critical errors
            if "stack_trace" not in kwargs:
                kwargs["stack_trace"] = traceback.format_exc()
            entry = self._create_log_entry(LogLevel.CRITICAL, message, **kwargs)
            self._add_to_buffer(entry)
    
    def security(self, message: str, **kwargs) -> None:
        """Log security event"""
        entry = self._create_log_entry(LogLevel.SECURITY, message, **kwargs)
        self._add_to_buffer(entry)
    
    def audit(self, message: str, **kwargs) -> None:
        """Log audit event"""
        entry = self._create_log_entry(LogLevel.AUDIT, message, **kwargs)
        self._add_to_buffer(entry)
    
    def _add_to_buffer(self, entry: LogEntry) -> None:
        """Add log entry to buffer or process immediately"""
        if self.config.buffer_size > 0:
            with self._buffer_lock:
                self._buffer.append(entry)
                if len(self._buffer) >= self.config.buffer_size:
                    self._flush_buffer()
        else:
            self._process_log_entry(entry, buffered=False)
    
    def flush(self) -> None:
        """Manually flush log buffer"""
        self._flush_buffer()
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get logging metrics"""
        return self._metrics.copy()
    
    def cleanup(self) -> None:
        """Cleanup logger resources"""
        try:
            # Signal shutdown
            self._shutdown_event.set()
            
            # Flush remaining logs
            self._flush_buffer()
            
            # Wait for flush task to complete
            if self._flush_task and self._flush_task.is_alive():
                self._flush_task.join(timeout=5)
            
            # Cleanup handlers
            for handler in self._handlers:
                if hasattr(handler, 'cleanup'):
                    handler.cleanup()
            
            logger.info("Logger cleanup completed", logger_name=self.name)
            
        except Exception as e:
            print(f"Error during logger cleanup: {str(e)}")

class LoggerManager:
    """Centralized logger management"""
    
    def __init__(self):
        self._loggers: Dict[str, StructuredLogger] = {}
        self._default_config = LogConfig()
        self._lock = threading.Lock()
    
    def get_logger(self, name: str, config: Optional[LogConfig] = None) -> StructuredLogger:
        """Get or create a logger instance"""
        with self._lock:
            if name not in self._loggers:
                logger_config = config or self._default_config
                self._loggers[name] = StructuredLogger(name, logger_config)
            return self._loggers[name]
    
    def configure_default(self, config: LogConfig) -> None:
        """Configure default logging settings"""
        self._default_config = config
    
    def get_all_loggers(self) -> Dict[str, StructuredLogger]:
        """Get all logger instances"""
        return self._loggers.copy()
    
    def cleanup_all(self) -> None:
        """Cleanup all loggers"""
        for logger_instance in self._loggers.values():
            logger_instance.cleanup()
        self._loggers.clear()

# ===============================================================================
# GLOBAL LOGGER MANAGER
# ===============================================================================

_logger_manager = LoggerManager()

def get_logger(name: str, config: Optional[LogConfig] = None) -> StructuredLogger:
    """Get a logger instance"""
    return _logger_manager.get_logger(name, config)

def configure_logging(config: LogConfig) -> None:
    """Configure global logging settings"""
    _logger_manager.configure_default(config)

# ===============================================================================
# UTILITY FUNCTIONS
# ===============================================================================

@asynccontextmanager
async def logging_context(
    correlation_id: Optional[str] = None,
    request_id: Optional[str] = None,
    user_context: Optional[Dict[str, Any]] = None
):
    """Context manager for adding logging context"""
    thread = threading.current_thread()
    
    # Store original values
    original_correlation_id = getattr(thread, 'correlation_id', None)
    original_request_id = getattr(thread, 'request_id', None)
    original_user_context = getattr(thread, 'user_context', None)
    
    try:
        # Set new context
        if correlation_id:
            thread.correlation_id = correlation_id
        if request_id:
            thread.request_id = request_id
        if user_context:
            thread.user_context = user_context
        
        yield
        
    finally:
        # Restore original values
        if original_correlation_id:
            thread.correlation_id = original_correlation_id
        elif hasattr(thread, 'correlation_id'):
            delattr(thread, 'correlation_id')
            
        if original_request_id:
            thread.request_id = original_request_id
        elif hasattr(thread, 'request_id'):
            delattr(thread, 'request_id')
            
        if original_user_context:
            thread.user_context = original_user_context
        elif hasattr(thread, 'user_context'):
            delattr(thread, 'user_context')

def with_performance_logging(func):
    """Decorator to add performance logging"""
    def wrapper(*args, **kwargs):
        start_time = datetime.utcnow()
        
        try:
            result = func(*args, **kwargs)
            
            end_time = datetime.utcnow()
            duration = (end_time - start_time).total_seconds()
            
            # Add performance metrics to thread context
            thread = threading.current_thread()
            thread.performance_metrics = {
                "function": func.__name__,
                "duration_seconds": duration,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat()
            }
            
            return result
            
        except Exception as e:
            end_time = datetime.utcnow()
            duration = (end_time - start_time).total_seconds()
            
            # Add error performance metrics
            thread = threading.current_thread()
            thread.performance_metrics = {
                "function": func.__name__,
                "duration_seconds": duration,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "error": str(e)
            }
            
            raise
    
    return wrapper

async def health_check() -> Dict[str, Any]:
    """Logging system health check"""
    try:
        all_loggers = _logger_manager.get_all_loggers()
        
        health_status = {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "module": "logging",
            "version": "4.0",
            "loggers_count": len(all_loggers),
            "loggers": {}
        }
        
        for name, logger_instance in all_loggers.items():
            metrics = logger_instance.get_metrics()
            health_status["loggers"][name] = {
                "total_logs": metrics["total_logs"],
                "errors_count": metrics["errors_count"],
                "last_flush": metrics["last_flush"].isoformat() if metrics["last_flush"] else None
            }
        
        return health_status
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "timestamp": datetime.utcnow().isoformat(),
            "error": str(e)
        }

# ===============================================================================
# MODULE INITIALIZATION
# ===============================================================================

async def initialize_logging_system(config: LogConfig) -> LoggerManager:
    """Initialize the logging system for production use"""
    try:
        # Configure global logging
        configure_logging(config)
        
        # Create default application logger
        app_logger = get_logger("ymera.application", config)
        
        # Log initialization success
        app_logger.info("Logging system initialized successfully")
        
        return _logger_manager
        
    except Exception as e:
        print(f"Failed to initialize logging system: {str(e)}")
        raise

# ===============================================================================
# EXPORTS
# ===============================================================================

__all__ = [
    "StructuredLogger",
    "LoggerManager",
    "LogConfig",
    "LogLevel",
    "LogEntry",
    "LogEntrySchema",
    "get_logger",
    "configure_logging",
    "logging_context",
    "with_performance_logging",
    "health_check",
    "initialize_logging_system"
]

# ===============================================================================
# logging/log_formatters.py
# ===============================================================================

"""
YMERA Enterprise - Log Formatters
Production-Ready Custom Log Formatting - v4.0
Enterprise-grade implementation with zero placeholders
"""

# ===============================================================================
# STANDARD IMPORTS SECTION
# ===============================================================================

import json
import logging
import traceback
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
from dataclasses import asdict
from abc import ABC, abstractmethod
import re
import hashlib

# Third-party imports
from colorama import Fore, Back, Style, init as colorama_init

# Local imports
from utils.encryption import encrypt_sensitive_data, is_sensitive_field

# ===============================================================================
# LOGGING CONFIGURATION
# ===============================================================================

colorama_init()  # Initialize colorama for cross-platform color support

# ===============================================================================
# CONSTANTS & CONFIGURATION
# ===============================================================================

# Color mapping for log levels
LEVEL_COLORS = {
    "TRACE": Fore.LIGHTBLACK_EX,
    "DEBUG": Fore.CYAN,
    "INFO": Fore.GREEN,
    "WARNING": Fore.YELLOW,
    "ERROR": Fore.RED,
    "CRITICAL": Fore.MAGENTA + Style.BRIGHT,
    "SECURITY": Fore.RED + Back.YELLOW + Style.BRIGHT,
    "AUDIT": Fore.BLUE + Style.BRIGHT
}

# Sensitive field patterns for data sanitization
SENSITIVE_PATTERNS = [
    r'password', r'passwd', r'pwd', r'secret', r'key', r'token',
    r'auth', r'credential', r'api_key', r'access_token', r'refresh_token',
    r'ssn', r'social_security', r'credit_card', r'card_number',
    r'email', r'phone', r'address', r'ip_address'
]

# ===============================================================================
# BASE FORMATTER CLASS
# ===============================================================================

class BaseLogFormatter(ABC):
    """Abstract base class for all log formatters"""
    
    def __init__(self, 
                 include_timestamp: bool = True,
                 include_level: bool = True,
                 include_logger_name: bool = True,
                 include_module: bool = True,
                 sanitize_sensitive: bool = True,
                 max_message_length: Optional[int] = None):
        self.include_timestamp = include_timestamp
        self.include_level = include_level
        self.include_logger_name = include_logger_name
        self.include_module = include_module
        self.sanitize_sensitive = sanitize_sensitive
        self.max_message_length = max_message_length
        
        # Compile sensitive patterns for performance
        self._sensitive_regex = re.compile(
            '|'.join(SENSITIVE_PATTERNS), 
            re.IGNORECASE
        ) if sanitize_sensitive else None
    
    @abstractmethod
    def format(self, entry: 'LogEntrySchema') -> str:
        """Format a log entry to string"""
        pass
    
    def _sanitize_data(self, data: Any) -> Any:
        """Sanitize sensitive data from log entries"""
        if not self.sanitize_sensitive:
            return data
        
        if isinstance(data, dict):
            sanitized = {}
            for key, value in data.items():
                if self._is_sensitive_field(key):
                    sanitized[key] = self._mask_sensitive_value(value)
                else:
                    sanitized[key] = self._sanitize_data(value)
            return sanitized
        
        elif isinstance(data, list):
            return [self._sanitize_data(item) for item in data]
        
        elif isinstance(data, str):
            if self._contains_sensitive_data(data):
                return self._mask_sensitive_string(data)
            return data
        
        return data
    
    def _is_sensitive_field(self, field_name: str) -> bool:
        """Check if field name indicates sensitive data"""
        return bool(self._sensitive_regex and self._sensitive_regex.search(field_name))
    
    def _contains_sensitive_data(self, text: str) -> bool:
        """Check if text contains sensitive patterns"""
        if not self._sensitive_regex:
            return False
        
        # Check for common sensitive patterns
        patterns = [
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN
            r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b',  # Credit card
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Email
            r'\b(?:\d{1,3}\.){3}\d{1,3}\b',  # IP address
            r'\b[A-Za-z0-9]{32,}\b'  # API keys/tokens
        ]
        
        return any(re.search(pattern, text) for pattern in patterns)
    
    def _mask_sensitive_value(self, value: Any) -> str:
        """Mask sensitive values"""
        if value is None:
            return None
        
        value_str = str(value)
        if len(value_str) <= 4:
            return "***"
        
        # Show first 2 and last 2 characters
        return f"{value_str[:2]}{'*' * (len(value_str) - 4)}{value_str[-2:]}"
    
    def _mask_sensitive_string(self, text: str) -> str:
        """Mask sensitive data in strings"""
        # Email addresses
        text = re.sub(
            r'\b([A-Za-z0-9._%+-]+)@([A-Za-z0-9.-]+\.[A-Z|a-z]{2,})\b',
            r'\1***@***.\2',
            text
        )
        
        # Credit card numbers
        text = re.sub(
            r'\b(\d{4})[\s-]?(\d{4})[\s-]?(\d{4})[\s-]?(\d{4})\b',
            r'\1-****-****-\4',
            text
        )
        
        # SSN
        text = re.sub(
            r'\b(\d{3})-(\d{2})-(\d{4})\b',
            r'\1-**-\3',
            text
        )
        
        return text
    
    def _truncate_message(self, message: str) -> str:
        """Truncate message if it exceeds maximum length"""
        if self.max_message_length and len(message) > self.max_message_length:
            return f"{message[:self.max_message_length-3]}..."
        return message

# ===============================================================================
# CONCRETE FORMATTER IMPLEMENTATIONS
# ===============================================================================

class JSONFormatter(BaseLogFormatter):
    """JSON log formatter for structured logging"""
    
    def __init__(self, 
                 compact: bool = False,
                 sort_keys: bool = False,
                 **kwargs):
        super().__init__(**kwargs)
        self.compact = compact
        self.sort_keys = sort_keys
    
    def format(self, entry: 'LogEntrySchema') -> str:
        """Format log entry as JSON"""
        try:
            # Convert entry to dictionary
            log_data = entry.dict()
            
            # Sanitize sensitive data
            log_data = self._sanitize_data(log_data)
            
            # Format timestamp as ISO string
            if isinstance(log_data.get('timestamp'), datetime):
                log_data['timestamp'] = log_data['timestamp'].isoformat()
            
            # Truncate message if needed
            if 'message' in log_data:
                log_data['message'] = self._truncate_message(log_data['message'])
            
            # Filter out None values and empty collections
            filtered_data = {
                k: v for k, v in log_data.items() 
                if v is not None and (not isinstance(v, (list, dict)) or v)
            }
            
            # Apply field inclusion filters
            if not self.include_timestamp:
                filtered_data.pop('timestamp', None)
            if not self.include_level:
                filtered_data.pop('level', None)
            if not self.include_logger_name:
                filtered_data.pop('logger_name', None)
            if not self.include_module:
                filtered_data.pop('module', None)
            
            # Format as JSON
            if self.compact:
                return json.dumps(
                    filtered_data, 
                    separators=(',', ':'),
                    sort_keys=self.sort_keys,
                    default=str
                )
            else:
                return json.dumps(
                    filtered_data,
                    indent=2,
                    sort_keys=self.sort_keys,
                    default=str
                )
                
        except Exception as e:
            # Fallback formatting if JSON serialization fails
            return json.dumps({
                "timestamp": datetime.utcnow().isoformat(),
                "level": "ERROR",
                "message": f"Log formatting error: {str(e)}",
                "original_message": str(entry.message) if hasattr(entry, 'message') else "Unknown"
            })

class ConsoleFormatter(BaseLogFormatter):
    """Human-readable console formatter with colors"""
    
    def __init__(self, 
                 use_colors: bool = True,
                 show_thread_info: bool = False,
                 show_performance: bool = True,
                 **kwargs):
        super().__init__(**kwargs)
        self.use_colors = use_colors
        self.show_thread_info = show_thread_info
        self.show_performance = show_performance
    
    def format(self, entry: 'LogEntrySchema') -> str:
        """Format log entry for console output"""
        try:
            parts = []
            
            # Timestamp
            if self.include_timestamp:
                timestamp = entry.timestamp.strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
                parts.append(f"[{timestamp}]")
            
            # Log level with color
            if self.include_level:
                level = entry.level.upper()
                if self.use_colors and level in LEVEL_COLORS:
                    colored_level = f"{LEVEL_COLORS[level]}{level:8}{Style.RESET_ALL}"
                    parts.append(f"[{colored_level}]")
                else:
                    parts.append(f"[{level:8}]")
            
            # Logger name
            if self.include_logger_name:
                parts.append(f"[{entry.logger_name}]")
            
            # Module and function
            if self.include_module:
                location = f"{entry.module}:{entry.function}:{entry.line_number}"
                parts.append(f"[{location}]")
            
            # Thread info
            if self.show_thread_info:
                parts.append(f"[T:{entry.thread_id}]")
            
            # Main message
            message = self._truncate_message(entry.message)
            message = self._sanitize_data(message)
            parts.append(message)
            
            # Performance metrics
            if self.show_performance and entry.performance_metrics:
                perf = entry.performance_metrics
                if 'duration_seconds' in perf:
                    duration = f"{perf['duration_seconds']:.3f}s"
                    parts.append(f"(took {duration})")
            
            # Extra data
            if entry.extra_data:
                sanitized_extra = self._sanitize_data(entry.extra_data)
                extra_str = " ".join([f"{k}={v}" for k, v in sanitized_extra.items()])
                parts.append(f"| {extra_str}")
            
            # Context information
            context_parts = []
            if entry.correlation_id:
                context_parts.append(f"correlation_id={entry.correlation_id}")
            if entry.request_id:
                context_parts.append(f"request_id={entry.request_id}")
            if entry.user_id:
                context_parts.append(f"user_id={entry.user_id}")
            
            if context_parts:
                parts.append(f"| {' '.join(context_parts)}")
            
            # Stack trace on new lines
            result = " ".join(parts)
            if entry.stack_trace:
                result += f"\n{entry.stack_trace}"
            
            return result
            
        except Exception as e:
            return f"[FORMATTING ERROR] {str(e)} | Original: {entry.message}"

class AuditFormatter(BaseLogFormatter):
    """Specialized formatter for audit trail logs"""
    
    def __init__(self, 
                 include_hash: bool = True,
                 hash_algorithm: str = "sha256",
                 **kwargs):
        super().__init__(**kwargs)
        self.include_hash = include_hash
        self.hash_algorithm = hash_algorithm
    
    def format(self, entry: 'LogEntrySchema') -> str:
        """Format audit log entry with integrity hash"""
        try:
            # Create base audit record
            audit_record = {
                "audit_id": str(uuid.uuid4()),
                "timestamp": entry.timestamp.isoformat(),
                "event_type": entry.level,
                "actor": entry.user_id or "system",
                "session_id": entry.session_id,
                "action": entry.message,
                "resource": entry.extra_data.get("resource"),
                "resource_id": entry.extra_data.get("resource_id"),
                "source_ip": entry.extra_data.get("source_ip"),
                "user_agent": entry.extra_data.get("user_agent"),
                "outcome": entry.extra_data.get("outcome", "unknown"),
                "details": entry.extra_data.get("details", {}),
                "correlation_id": entry.correlation_id,
                "module": entry.module,
                "function": entry.function
            }
            
            # Remove None values
            audit_record = {k: v for k, v in audit_record.items() if v is not None}
            
            # Add integrity hash
            if self.include_hash:
                record_string = json.dumps(audit_record, sort_keys=True, default=str)
                hash_obj = hashlib.new(self.hash_algorithm)
                hash_obj.update(record_string.encode('utf-8'))
                audit_record["integrity_hash"] = hash_obj.hexdigest()
            
            return json.dumps(audit_record, default=str)
            
        except Exception as e:
            return json.dumps({
                "audit_id": str(uuid.uuid4()),
                "timestamp": datetime.utcnow().isoformat(),
                "event_type": "ERROR",
                "action": f"Audit formatting error: {str(e)}",
                "original_message": str(entry.message)
            })

class SecurityFormatter(BaseLogFormatter):
    """Specialized formatter for security events"""
    
    def __init__(self, 
                 include_geolocation: bool = True,
                 include_risk_score: bool = True,
                 **kwargs):
        super().__init__(**kwargs)
        self.include_geolocation = include_geolocation
        self.include_risk_score = include_risk_score
    
    def format(self, entry: 'LogEntrySchema') -> str:
        """Format security log entry with enhanced context"""
        try:
            security_context = entry.security_context or {}
            
            security_record = {
                "security_event_id": str(uuid.uuid4()),
                "timestamp": entry.timestamp.isoformat(),
                "severity": entry.level,
                "event_category": security_context.get("category", "unknown"),
                "event_type": security_context.get("event_type", entry.message),
                "threat_level": security_context.get("threat_level", "unknown"),
                "source": {
                    "ip_address": self._mask_ip_address(security_context.get("source_ip")),
                    "user_agent": security_context.get("user_agent"),
                    "country": security_context.get("country") if self.include_geolocation else None,
                    "city": security_context.get("city") if self.include_geolocation else None
                },
                "target": {
                    "user_id": entry.user_id,
                    "session_id": entry.session_id,
                    "resource": security_context.get("target_resource"),
                    "endpoint": security_context.get("target_endpoint")
                },
                "attack_details": {
                    "attack_type": security_context.get("attack_type"),
                    "attack_vector": security_context.get("attack_vector"),
                    "payload": self._sanitize_data(security_context.get("payload")),
                    "blocked": security_context.get("blocked", False)
                },
                "risk_assessment": {
                    "risk_score": security_context.get("risk_score") if self.include_risk_score else None,
                    "confidence": security_context.get("confidence"),
                    "indicators": security_context.get("indicators", [])
                },
                "response": {
                    "action_taken": security_context.get("action_taken"),
                    "escalated": security_context.get("escalated", False),
                    "incident_id": security_context.get("incident_id")
                },
                "correlation_id": entry.correlation_id,
                "investigation_notes": security_context.get("notes")
            }
            
            # Remove None values and empty structures
            security_record = self._clean_record(security_record)
            
            return json.dumps(security_record, default=str, indent=2)
            
        except Exception as e:
            return json.dumps({
                "security_event_id": str(uuid.uuid4()),
                "timestamp": datetime.utcnow().isoformat(),
                "severity": "ERROR",
                "event_type": f"Security log formatting error: {str(e)}",
                "original_message": str(entry.message)
            })
    
    def _mask_ip_address(self, ip_address: Optional[str]) -> Optional[str]:
        """Mask IP address for privacy compliance"""
        if not ip_address:
            return None
        
        if self.sanitize_sensitive:
            # Mask last octet for IPv4
            if '.' in ip_address:
                parts = ip_address.split('.')
                if len(parts) == 4:
                    return f"{parts[0]}.{parts[1]}.{parts[2]}.***"
            # Mask last segments for IPv6
            elif ':' in ip_address:
                parts = ip_address.split(':')
                if len(parts) >= 4:
                    return ':'.join(parts[:-2]) + ':***:***'
        
        return ip_address
    
    def _clean_record(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """Remove None values and empty structures recursively"""
        if isinstance(record, dict):
            cleaned = {}
            for key, value in record.items():
                if value is not None:
                    if isinstance(value, dict):
                        cleaned_value = self._clean_record(value)
                        if cleaned_value:  # Only add non-empty dicts
                            cleaned[key] = cleaned_value
                    elif isinstance(value, list):
                        cleaned_value = [self._clean_record(item) if isinstance(item, dict) else item 
                                       for item in value if item is not None]
                        if cleaned_value:  # Only add non-empty lists
                            cleaned[key] = cleaned_value
                    else:
                        cleaned[key] = value
            return cleaned
        return record

class PerformanceFormatter(BaseLogFormatter):
    """Specialized formatter for performance monitoring logs"""
    
    def __init__(self, 
                 include_system_metrics: bool = True,
                 include_trace_data: bool = False,
                 **kwargs):
        super().__init__(**kwargs)
        self.include_system_metrics = include_system_metrics
        self.include_trace_data = include_trace_data
    
    def format(self, entry: 'LogEntrySchema') -> str:
        """Format performance monitoring log entry"""
        try:
            perf_metrics = entry.performance_metrics or {}
            
            performance_record = {
                "performance_id": str(uuid.uuid4()),
                "timestamp": entry.timestamp.isoformat(),
                "operation": perf_metrics.get("function", entry.function),
                "module": entry.module,
                "execution_time": {
                    "start_time": perf_metrics.get("start_time"),
                    "end_time": perf_metrics.get("end_time"),
                    "duration_seconds": perf_metrics.get("duration_seconds"),
                    "duration_ms": perf_metrics.get("duration_seconds", 0) * 1000
                },
                "performance_metrics": {
                    "cpu_usage": perf_metrics.get("cpu_usage"),
                    "memory_usage": perf_metrics.get("memory_usage"),
                    "disk_io": perf_metrics.get("disk_io"),
                    "network_io": perf_metrics.get("network_io"),
                    "database_queries": perf_metrics.get("database_queries"),
                    "cache_hits": perf_metrics.get("cache_hits"),
                    "cache_misses": perf_metrics.get("cache_misses")
                } if self.include_system_metrics else None,
                "context": {
                    "user_id": entry.user_id,
                    "session_id": entry.session_id,
                    "correlation_id": entry.correlation_id,
                    "request_id": entry.request_id
                },
                "result": {
                    "success": perf_metrics.get("error") is None,
                    "error": perf_metrics.get("error"),
                    "status_code": perf_metrics.get("status_code"),
                    "response_size": perf_metrics.get("response_size")
                },
                "trace_data": perf_metrics.get("trace_data") if self.include_trace_data else None
            }
            
            # Remove None values and empty structures
            performance_record = self._clean_record(performance_record)
            
            return json.dumps(performance_record, default=str)
            
        except Exception as e:
            return json.dumps({
                "performance_id": str(uuid.uuid4()),
                "timestamp": datetime.utcnow().isoformat(),
                "operation": "performance_log_formatting_error",
                "error": str(e),
                "original_message": str(entry.message)
            })
    
    def _clean_record(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """Remove None values and empty structures recursively"""
        if isinstance(record, dict):
            cleaned = {}
            for key, value in record.items():
                if value is not None:
                    if isinstance(value, dict):
                        cleaned_value = self._clean_record(value)
                        if cleaned_value:
                            cleaned[key] = cleaned_value
                    elif isinstance(value, list):
                        cleaned_value = [self._clean_record(item) if isinstance(item, dict) else item 
                                       for item in value if item is not None]
                        if cleaned_value:
                            cleaned[key] = cleaned_value
                    else:
                        cleaned[key] = value
            return cleaned
        return record

# ===============================================================================
# UTILITY FUNCTIONS
# ===============================================================================

def create_formatter(
    formatter_type: str,
    **kwargs
) -> BaseLogFormatter:
    """Factory function to create formatters"""
    formatters = {
        "json": JSONFormatter,
        "console": ConsoleFormatter,
        "audit": AuditFormatter,
        "security": SecurityFormatter,
        "performance": PerformanceFormatter
    }
    
    if formatter_type not in formatters:
        raise ValueError(f"Unknown formatter type: {formatter_type}")
    
    return formatters[formatter_type](**kwargs)

def get_default_formatter_config(formatter_type: str) -> Dict[str, Any]:
    """Get default configuration for formatter types"""
    configs = {
        "json": {
            "compact": False,
            "sort_keys": True,
            "sanitize_sensitive": True
        },
        "console": {
            "use_colors": True,
            "show_thread_info": False,
            "show_performance": True,
            "sanitize_sensitive": True
        },
        "audit": {
            "include_hash": True,
            "hash_algorithm": "sha256",
            "sanitize_sensitive": False  # Audit logs need complete data
        },
        "security": {
            "include_geolocation": True,
            "include_risk_score": True,
            "sanitize_sensitive": True
        },
        "performance": {
            "include_system_metrics": True,
            "include_trace_data": False,
            "sanitize_sensitive": True
        }
    }
    
    return configs.get(formatter_type, {})

# ===============================================================================
# EXPORTS
# ===============================================================================

__all__ = [
    "BaseLogFormatter",
    "JSONFormatter",
    "ConsoleFormatter", 
    "AuditFormatter",
    "SecurityFormatter",
    "PerformanceFormatter",
    "create_formatter",
    "get_default_formatter_config",
    "LEVEL_COLORS",
    "SENSITIVE_PATTERNS"
]